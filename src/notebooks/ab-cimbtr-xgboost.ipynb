{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":70942,"databundleVersionId":10381525,"sourceType":"competition"}],"dockerImageVersionId":30840,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport wandb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\nfrom sklearn.metrics import accuracy_score, roc_auc_score, classification_report\nfrom xgboost import XGBClassifier\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:05:19.012980Z","iopub.execute_input":"2025-01-18T19:05:19.013263Z","iopub.status.idle":"2025-01-18T19:05:20.460828Z","shell.execute_reply.started":"2025-01-18T19:05:19.013238Z","shell.execute_reply":"2025-01-18T19:05:20.460199Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# load the data\nraw_df_train = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/train.csv\")\nraw_df_test = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:05:22.069235Z","iopub.execute_input":"2025-01-18T19:05:22.069719Z","iopub.status.idle":"2025-01-18T19:05:22.306277Z","shell.execute_reply.started":"2025-01-18T19:05:22.069693Z","shell.execute_reply":"2025-01-18T19:05:22.305576Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# differentiate the categorical and numerical cols\ndef identify_numerical_catergorical_columns(df):\n    \"\"\"\n    Identify the Numerical and the Categorical Columns in the dataframe\n    \"\"\"\n    numerical_cols = df.select_dtypes(include=['number']).columns.tolist()\n    categorical_cols = df.select_dtypes(exclude=['number']).columns.tolist()\n\n    return numerical_cols, categorical_cols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:05:23.547272Z","iopub.execute_input":"2025-01-18T19:05:23.547706Z","iopub.status.idle":"2025-01-18T19:05:23.551985Z","shell.execute_reply.started":"2025-01-18T19:05:23.547670Z","shell.execute_reply":"2025-01-18T19:05:23.551192Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# create data pre procesing pipeline\ndef get_data_transformer_object(numerical_cols, categorical_cols):\n    num_pipeline = Pipeline(\n                steps=[\n                    ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n                    ('standar_scaler', StandardScaler())\n                    \n                ]\n            )\n\n    cat_pipeline = Pipeline(\n                steps = [\n                    ('imputer', SimpleImputer(strategy='constant', fill_value='other')),\n                    (\"label_encoder\", OrdinalEncoder())\n                ]\n            )\n\n    preprocessor = ColumnTransformer(\n                [\n                    (\"num\", num_pipeline, numerical_cols),\n                    (\"cat\", cat_pipeline, categorical_cols)\n                ]\n            )\n\n    return preprocessor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:39:20.553560Z","iopub.execute_input":"2025-01-18T19:39:20.553883Z","iopub.status.idle":"2025-01-18T19:39:20.558646Z","shell.execute_reply.started":"2025-01-18T19:39:20.553858Z","shell.execute_reply":"2025-01-18T19:39:20.557817Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# final data creation step\ntarget = 'efs'\ndf_features = raw_df_train.drop(columns=[target, 'ID', 'efs_time'], axis = 1)\nprint(\"df_features shape : {}\".format(df_features.shape))\n\n\ndf_label  = np.array(raw_df_train[target]).reshape(-1, 1)\nprint(\"Train Label Shape  : {}\".format(df_label.shape))\n\nnumerical_cols, categorical_cols = identify_numerical_catergorical_columns(df_features)\nprint(\"----- Numerical columns -------\")\nprint(numerical_cols)\nprint(\"----- Categorical columns -------\")\nprint(categorical_cols)\npreprocesser_obj = get_data_transformer_object(numerical_cols, categorical_cols)\n\nprocessed_df_arr = preprocesser_obj.fit_transform(df_features)\n\nX_train, X_valid, y_train, y_valid = train_test_split(processed_df_arr, df_label, test_size=0.2, random_state=42)\n\nprint(X_train.shape)\nprint(X_valid.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:39:22.690768Z","iopub.execute_input":"2025-01-18T19:39:22.691094Z","iopub.status.idle":"2025-01-18T19:39:23.044744Z","shell.execute_reply.started":"2025-01-18T19:39:22.691066Z","shell.execute_reply":"2025-01-18T19:39:23.043995Z"}},"outputs":[{"name":"stdout","text":"df_features shape : (28800, 57)\nTrain Label Shape  : (28800, 1)\n----- Numerical columns -------\n['hla_match_c_high', 'hla_high_res_8', 'hla_low_res_6', 'hla_high_res_6', 'hla_high_res_10', 'hla_match_dqb1_high', 'hla_nmdp_6', 'hla_match_c_low', 'hla_match_drb1_low', 'hla_match_dqb1_low', 'year_hct', 'hla_match_a_high', 'donor_age', 'hla_match_b_low', 'age_at_hct', 'hla_match_a_low', 'hla_match_b_high', 'comorbidity_score', 'karnofsky_score', 'hla_low_res_8', 'hla_match_drb1_high', 'hla_low_res_10']\n----- Categorical columns -------\n['dri_score', 'psych_disturb', 'cyto_score', 'diabetes', 'tbi_status', 'arrhythmia', 'graft_type', 'vent_hist', 'renal_issue', 'pulm_severe', 'prim_disease_hct', 'cmv_status', 'tce_imm_match', 'rituximab', 'prod_type', 'cyto_score_detail', 'conditioning_intensity', 'ethnicity', 'obesity', 'mrd_hct', 'in_vivo_tcd', 'tce_match', 'hepatic_severe', 'prior_tumor', 'peptic_ulcer', 'gvhd_proph', 'rheum_issue', 'sex_match', 'race_group', 'hepatic_mild', 'tce_div_match', 'donor_related', 'melphalan_dose', 'cardiac', 'pulm_moderate']\n(23040, 57)\n(5760, 57)\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# create model instance\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold \nimport gc\nfolds = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\nvalidation_score_arr = np.zeros(X_train.shape[0])\nxgboost_model = XGBClassifier(\n    n_estimators=100,       # Number of trees\n    learning_rate=0.1,      # Step size shrinkage\n    max_depth=4,            # Maximum tree depth\n    objective='binary:logistic',  # For binary classification\n    eval_metric='logloss',# Evaluation metric\n    enable_categorical=True,\n    random_state=42,\n    tree_method='hist',\n    device='cuda',\n    early_stopping_rounds=10,\n)\n\n\nfor fold_index, (train_index,val_index) in enumerate(folds.split(X_train,y_train)):\n    print(\"Train Index : \", type(train_index))\n    print(\"Validation Index : \", type(val_index))\n    print('Batch {} started...'.format(fold_index))\n    gc.collect()\n    bst = xgboost_model.fit(X_train[train_index],y_train[train_index],\n              eval_set = [(X_train[val_index],y_train[val_index])],\n              verbose= 200\n              )\n\n    validation_score_arr[val_index] = xgboost_model.predict_proba(X_train[val_index])[:, 1]\n    print(validation_score_arr[val_index])\n    validation_score_arr[val_index] = (validation_score_arr[val_index] >= 0.5).astype(int)\n    accuracy = accuracy_score(y_train[val_index], validation_score_arr[val_index])\n    print(\"Accuracy_score : {}\".format(accuracy))\n\"\"\"\n# fit model\nxgboost_baseline_model.fit(X_train, y_train)\n# make predictions\npreds = xgboost_baseline_model.predict(X_valid)\ny_pred_proba = xgboost_baseline_model.predict_proba(X_valid)[:, 1] \naccuracy_score(y_valid, preds)\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:39:27.721074Z","iopub.execute_input":"2025-01-18T19:39:27.721451Z","iopub.status.idle":"2025-01-18T19:39:28.885791Z","shell.execute_reply.started":"2025-01-18T19:39:27.721423Z","shell.execute_reply":"2025-01-18T19:39:28.885092Z"}},"outputs":[{"name":"stdout","text":"Train Index :  <class 'numpy.ndarray'>\nValidation Index :  <class 'numpy.ndarray'>\nBatch 0 started...\n[0]\tvalidation_0-logloss:0.67868\n[99]\tvalidation_0-logloss:0.58867\n[0.67874253 0.6348446  0.58607972 ... 0.65730953 0.67232025 0.53507066]\nAccuracy_score : 0.6869791666666667\nTrain Index :  <class 'numpy.ndarray'>\nValidation Index :  <class 'numpy.ndarray'>\nBatch 1 started...\n[0]\tvalidation_0-logloss:0.67890\n[99]\tvalidation_0-logloss:0.59166\n[0.33418384 0.7266981  0.64476806 ... 0.77162361 0.59264064 0.67738056]\nAccuracy_score : 0.6829427083333334\nTrain Index :  <class 'numpy.ndarray'>\nValidation Index :  <class 'numpy.ndarray'>\nBatch 2 started...\n[0]\tvalidation_0-logloss:0.67855\n[99]\tvalidation_0-logloss:0.59539\n[0.71849811 0.69594926 0.70964676 ... 0.72905672 0.56391186 0.33038661]\nAccuracy_score : 0.6825520833333333\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"'\\n# fit model\\nxgboost_baseline_model.fit(X_train, y_train)\\n# make predictions\\npreds = xgboost_baseline_model.predict(X_valid)\\ny_pred_proba = xgboost_baseline_model.predict_proba(X_valid)[:, 1] \\naccuracy_score(y_valid, preds)\\n'"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"test_df = raw_df_test.drop(columns = ['ID'], axis = 1)\nnumerical_cols_test, categorical_cols_test = identify_numerical_catergorical_columns(df_features)\nprint(numerical_cols)\nprint(categorical_cols_test)\npreprocesser_obj_test = get_data_transformer_object(numerical_cols_test, categorical_cols_test)\nprocessed_test_arr = preprocesser_obj_test.fit_transform(test_df)\nprocessed_test_arr.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:20:34.266881Z","iopub.execute_input":"2025-01-18T19:20:34.267257Z","iopub.status.idle":"2025-01-18T19:20:34.293738Z","shell.execute_reply.started":"2025-01-18T19:20:34.267226Z","shell.execute_reply":"2025-01-18T19:20:34.292981Z"}},"outputs":[{"name":"stdout","text":"['hla_match_c_high', 'hla_high_res_8', 'hla_low_res_6', 'hla_high_res_6', 'hla_high_res_10', 'hla_match_dqb1_high', 'hla_nmdp_6', 'hla_match_c_low', 'hla_match_drb1_low', 'hla_match_dqb1_low', 'year_hct', 'hla_match_a_high', 'donor_age', 'hla_match_b_low', 'age_at_hct', 'hla_match_a_low', 'hla_match_b_high', 'comorbidity_score', 'karnofsky_score', 'hla_low_res_8', 'hla_match_drb1_high', 'hla_low_res_10']\n['dri_score', 'psych_disturb', 'cyto_score', 'diabetes', 'tbi_status', 'arrhythmia', 'graft_type', 'vent_hist', 'renal_issue', 'pulm_severe', 'prim_disease_hct', 'cmv_status', 'tce_imm_match', 'rituximab', 'prod_type', 'cyto_score_detail', 'conditioning_intensity', 'ethnicity', 'obesity', 'mrd_hct', 'in_vivo_tcd', 'tce_match', 'hepatic_severe', 'prior_tumor', 'peptic_ulcer', 'gvhd_proph', 'rheum_issue', 'sex_match', 'race_group', 'hepatic_mild', 'tce_div_match', 'donor_related', 'melphalan_dose', 'cardiac', 'pulm_moderate']\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"(3, 57)"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"submission_prob = np.max(xgboost_model.predict_proba(processed_test_arr), axis = 1)\nprint(submission_prob)\nsubmission_df = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv\")\nsubmission_df.head()\nsubmission_df[\"prediction\"] = submission_prob\nsubmission_df.to_csv(\"/kaggle/working/submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:21:01.721875Z","iopub.execute_input":"2025-01-18T19:21:01.722209Z","iopub.status.idle":"2025-01-18T19:21:01.746020Z","shell.execute_reply.started":"2025-01-18T19:21:01.722180Z","shell.execute_reply":"2025-01-18T19:21:01.745072Z"}},"outputs":[{"name":"stdout","text":"[0.7677845  0.73663646 0.7753409 ]\n","output_type":"stream"}],"execution_count":26}]}